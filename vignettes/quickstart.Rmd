---
title: "Quick Start Guide for keyclust"
author: "Patrick Chester"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Quick Start Guide for keyclust}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction
The `keyclust` algorithm is designed to allow users to produce keyword sets of relevance to concepts of interest given a fitted word embedding model and seed words representing the target concepts. 

In this guide we will cover how to apply `keyclust` to two English-language embedding models from FastText and GloVe, as well as a Chinese-language embedding model from FastText.


## Load libraries
```{r, warn=FALSE, message=FALSE}
rm(list = ls())

library(R.utils)
library(keyclust)
library(data.table)
library(knitr)

embedding_rows <- 20000
```

*Note:* The resources required to compute cosine similarity increases quadratically with the number of words in the embeddings input. For example, if you are computing similarities between 10 terms, that will require 10^2 = 100 computations. For 20 terms, it will be 400 calculations! For this reason, I don’t suggest that you input embedding models with more than 20k terms on a standard laptop (with 16gb of ram) into this function. Though, higher numbers of terms will typically allow you to target more precise word clusters.

### Data download

Here we download three embedding models from FastText and GloVe for use as inputs to `keyclust`. These models have been tested and confirmed to work by the author of this package.

```{r}
options(timeout = 400)

# For English language embeddings produced by the FastText Model
if (!file.exists("cc.en.300.vec")) {
  download.file("https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz",  destfile = "cc.en.300.vec.gz")
  gunzip("cc.en.300.vec.gz")
}

# For English language embeddings produced by the GloVe Model

if (!file.exists("glove.6B.300d.txt")) {
  download.file("https://nlp.stanford.edu/data/glove.6B.zip", destfile = "glove6B.zip")
  unzip("glove6B.zip")
}

# For Chinese language embeddings produced by the FastText Model
if (!file.exists("cc.zh.300.vec")) {
  download.file("https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz",  destfile = "cc.zh.300.vec.gz")
  gunzip("cc.zh.300.vec.gz")
}
```


## FastText Embeddings with English Language

```{r}
ft_embed <- fread("cc.en.300.vec", nrows = embedding_rows, skip = 1, quote = "")
names(ft_embed) <- c("words", paste0("V", 1:300)) # Default column names are not great
```

### Checking dimensions

```{r}
dim(ft_embed)
```

### Inspecting the data

```{r}
head(ft_embed[, 1:10]) |> 
  kable()
```

This is generally what a fitted embedding model should look like. The columns are the embedding weights and the rows are the tokens that correspond to the embedding weights. The industry standard is to fit models with 300 embedding dimensions, which is also true of this FastText model.


```{r}
cat(ft_embed$words[1:100])
```

The embeddings matrix is ordered by frequency. It also includes punctuation and variations of capitalization.

### Preprocessing Embeddings

Let's remove punctuation and capitalization for this first exercise. Let's also lemmatize to combine similar terms.

```{r}
ft_embed_processed <- process_embed(ft_embed, words = "words", punct = TRUE, tolower = TRUE, lemmatize = TRUE)
```

```{r}
dim(ft_embed_processed)
```

We can see that we've reduced the number of terms from 20k to ~11k.

```{r}
cat(ft_embed_processed$words[1:100])
```

The remaining words appear to be more useful, as there are few with duplicated semantic meaning. I.e. running and run should be treated roughly the same as semantic entities.

### Creating a cosine similarity matrix

`keyclust` takes as an input a matrix representing the cosine similarity between each pairing of words.

```{r}
ft_embed_sim_mat <- similarity_matrix(ft_embed_processed, words = "words")
```


```{r}
str(ft_embed_sim_mat)
```

We can see that the output of this function is a square cosine similarity matrix.

### Seeds for target concept: Peace

```{r}
peace <- c("peace", "harmony", "tranquility")
```

### Running the keyclust model
```{r}
keyclust_peace_output <- keyclust(sim_mat = ft_embed_sim_mat,
                                  seed_words = peace,
                                  max_n = 20)
```

*Note:* In this example `keyclust` restricted to producing a maximum output of 20 terms

```{r}
keyclust_peace_output |>
  terms() |>
  kable()
```


### Seeds for target concept: Democracy
```{r}
democracy <- c("election", "vote", "campaign", "candidate")
```

### Running the keyclust model
```{r}
keyclust_democracy_output <- keyclust(sim_mat = ft_embed_sim_mat,
                                      seed_words = democracy,
                                      max_n = 20)
```

```{r}
keyclust_democracy_output |>
  terms() |>
  kable()
```

```{r}
rm(list = c("ft_embed", "ft_embed_processed"))
gc()
```


## GloVe Embeddings with English Language

`keyclust` will work no matter what algorithm produced the embeddings, so long as they are in the proper format (embeddings as columns and words as rows). Below, I replicate the results for "peace" with GloVe embeddings.

```{r}
gl_embed <- fread("glove.6B.300d.txt", nrows = embedding_rows, header = TRUE, quote = "")
names(gl_embed) <- c("words", paste0("V", 1:300)) # Default column names are not great
```

```{r}
dim(gl_embed)
```

### Running the keyclust model
```{r}
keyclust_peace_output <-
  gl_embed |>
  process_embed(words = "words") |>
  similarity_matrix(words = "words") |>
  keyclust(seed_words = peace, max_n = 20)
```

```{r}
keyclust_peace_output |>
  terms() |>
  kable()
```

```{r}
rm(list = "gl_embed")
gc()
```

*Note:* The results are different, because of two main reasons:
1. These two models were fit on different corpora
2. The algorithms used to generate the embeddings have different properties. E.x. FastText tends to perform better for rarely occurring terms

## FastText Embeddings with Chinese Language

`keyclust` can be used to produce keyword sets from embeddings fit on any language! In this case, I generate a new "peace" dictionary using FastText Chinese embeddings.

```{r}
ft_cn_embed <- fread("cc.zh.300.vec", nrows = embedding_rows, skip = 1, quote = "")
names(ft_cn_embed) <- c("words", paste0("V", 1:300)) # Default column names are not great
```

```{r}
head(ft_cn_embed[, 1:10]) |>
  kable()
```

### Seeds for target concept: Peace
```{r}
peace_cn <- c("和平", # he2ping2, peace
              "平安", # ping2an1, safe
              "和谐") # he2xie2, harmonious
```

### Running the keyclust model
```{r}
keyclust_peace_output <-
  ft_cn_embed |>
  process_embed(words = "words") |>
  similarity_matrix(words = "words") |>
  keyclust(seed_words = peace_cn,
           #max_n = 20,
           sim_thresh = 0.4)
```

*Note:* Removing capitalization and lemmatization are not relevant for Chinese, so in this case `process_embed` only converts the data.table to the optimal format for subsequent functions

```{r}
keyclust_peace_output |>
  terms() |>
  kable()
```

## Final notes

Feel free to use `keyclust` to generate keywords of your own. I would be grateful for any feedback or suggestions.
